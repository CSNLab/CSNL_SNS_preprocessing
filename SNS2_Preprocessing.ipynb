{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dorm Study: SNS2 Clean-up\n",
    "## Karina Lopez\n",
    "## 08/14/2020\n",
    "## This script will clean the second-wave social network survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "\n",
    "#setting pandas display options\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199\n",
    "pd.set_option('display.precision',20)\n",
    "\n",
    "BASE_DIR = \"/Users/karina/Box Sync/Cleaningup_SNS2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function filters a dataframe based on a filter dataset that contains string ids and a boolean True\n",
    "def filter_this(filter_df, to_filter_df, rem_col_name='remove'):\n",
    "    \n",
    "    new_df = pd.merge(to_filter_df, \n",
    "             filter_df, \n",
    "             how='outer', on='id')\n",
    "\n",
    "    # Fill NAs in new column w/ False\n",
    "    new_df = new_df.fillna(value=0)\n",
    "\n",
    "    new_df = new_df.loc[new_df[rem_col_name] == False]\n",
    "    del new_df[rem_col_name]\n",
    "    \n",
    "    return(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to identify empty cells\n",
    "\n",
    "def where_empty(df, p_df = False, p_list = False):\n",
    "    is_NaN = df.isnull()\n",
    "    row_has_NaN = is_NaN.any(axis=1)\n",
    "    rows_with_NaN = df[row_has_NaN]\n",
    "    \n",
    "    list_missing_egos = rows_with_NaN[\"ego\"].values.tolist()\n",
    "    \n",
    "    if p_df:\n",
    "        print(rows_with_NaN)\n",
    "    \n",
    "    if p_list:\n",
    "        print(list_missing_egos)\n",
    "    \n",
    "    return(list_missing_egos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgelist_creator(network_wing_df):\n",
    "\n",
    "    #include section that create list of all_friend_headers\n",
    "    all_friend_headers = list(network_wing_df.columns)\n",
    "    all_friend_headers  = all_friend_headers[6:]\n",
    "    \n",
    "    wing_edgelist_dorm_df = pd.DataFrame()\n",
    "    final_wing_dorm_df = pd.DataFrame()\n",
    "\n",
    "    #for loop for each dorm header\n",
    "    for friend_column in all_friend_headers:\n",
    "        wing_edgelist_dorm_df = pd.DataFrame()\n",
    "        list_connections = []\n",
    "\n",
    "        #for loop for each ego and alter for that column\n",
    "        for column in network_wing_df[['ego', str(friend_column)]]:\n",
    "            #create a series object\n",
    "            columnSeriesObj = network_wing_df[column]       \n",
    "\n",
    "            #add alter and ego series to dataframe\n",
    "            wing_edgelist_dorm_df = wing_edgelist_dorm_df.append(columnSeriesObj)\n",
    "\n",
    "        #create column holding repeating dorm/outside\n",
    "        list_connections.append(friend_column[0:22])\n",
    "        list_connections = list_connections * wing_edgelist_dorm_df.shape[1]\n",
    "        wing_edgelist_dorm_df.loc[len(wing_edgelist_dorm_df), :] = list_connections\n",
    "\n",
    "        #transpose dataframe and add to final edgelist\n",
    "        wing_edgelist_dorm_df = wing_edgelist_dorm_df.transpose()\n",
    "        wing_edgelist_dorm_df.columns = ['ego', 'alter', 'connection']\n",
    "        final_wing_dorm_df = final_wing_dorm_df.append(wing_edgelist_dorm_df)\n",
    "\n",
    "\n",
    "    # drop all connections with NA in alter and replace outs with outside\n",
    "    final_wing_dorm_df = final_wing_dorm_df[final_wing_dorm_df != str(0)].dropna()\n",
    "\n",
    "    # reset the index\n",
    "    final_wing_dorm_df = final_wing_dorm_df.reset_index(drop = True)\n",
    "    \n",
    "    # lowercase everything and remove trailing zeros\n",
    "    final_wing_dorm_df['alter'] = final_wing_dorm_df['alter'].str.lower()\n",
    "    final_wing_dorm_df['alter']= final_wing_dorm_df['alter'].str.strip()\n",
    "    \n",
    "    return(final_wing_dorm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dataframe containing ego, alter, alter_id, connection, id\n",
    "def alter_anonymizer_df(df):\n",
    "    \n",
    "    df = df[['ego', 'alter', 'connection', 'dorm_wing_x', 'id']].copy()\n",
    "\n",
    "    #split the string by space\n",
    "    new_data = df[\"alter\"].str.split(\" \", n = 1, expand = True)\n",
    "    df[\"alter_first\"] = new_data[0]\n",
    "    df[\"alter_last\"] = new_data[1]\n",
    "\n",
    "    #create new alter id (first name last name initial)\n",
    "    new_data = df[\"alter_first\"] + \" \" + df[\"alter_last\"].str[0]\n",
    "    df['alter_id'] = new_data\n",
    "\n",
    "    #drop old columns\n",
    "    df = df.drop(columns=['alter_first', 'alter_last'])\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dataframe containing ego, alter, alter_id, connection\n",
    "\n",
    "def alter_df_func(df, name_col = 'name', alter_col = 'alter', alter_id_col = 'alter_id', connection_col = 'connection', drop_extra = True):\n",
    "\n",
    "    alter_df = df[['ego', name_col]].copy()\n",
    "\n",
    "    # lowercase the names\n",
    "    df[name_col] = df[name_col].str.lower()\n",
    "\n",
    "    # separate the alter name by space and create new column with first name\n",
    "    new_data = df[name_col].str.split(' ', n = 1, expand = True)\n",
    "    alter_df['alter_first'] = new_data[0]\n",
    "\n",
    "    # create column for alter connection\n",
    "    alter_df[connection_col] = new_data[1].str[-4:]\n",
    "    # rename the connections\n",
    "    alter_df[connection_col] = alter_df[connection_col].map({'-2n)': 'outside', '-2s)': 'outside', '(2s)': 'dorm', '(2n)':'dorm'})\n",
    "\n",
    "    # create column for alter last name\n",
    "    alter_df.loc[(alter_df[connection_col] == 'dorm'), 'alter_last'] = new_data[1].str[:-5]\n",
    "    alter_df.loc[(alter_df[connection_col] == 'outside'), 'alter_last'] = new_data[1].str[:-9]\n",
    "\n",
    "    # join first and last names\n",
    "    alter_df[alter_col] = alter_df[\"alter_first\"] + \" \" + alter_df[\"alter_last\"]\n",
    "\n",
    "    # create column for alter_id\n",
    "    alter_df[alter_id_col] = alter_df[\"alter_first\"] + \" \" + alter_df[\"alter_last\"].str[0]\n",
    "\n",
    "    if drop_extra:\n",
    "        alter_df = alter_df.drop(columns=['alter_first', 'alter_last', name_col])\n",
    "\n",
    "    return(alter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_strings(df, current = False):\n",
    "    df = df.dropna(subset = ['question'])\n",
    "    \n",
    "    df.loc[df['question'].str.contains('Pre-COVID, how often'), 'question'] = 'preCOV_interact'\n",
    "    df.loc[df['question'].str.contains('During-COVID, how of'), 'question'] = 'durCOV_interact'\n",
    "    df.loc[df['question'].str.contains('Pre-COVID, how close'), 'question'] = 'preCOV_closeness'\n",
    "    df.loc[df['question'].str.contains('During-COVID, how cl'), 'question'] = 'durCOV_closeness'\n",
    "    df.loc[df['question'].str.contains('How far away did you'), 'question'] = 'preCOV_prox'\n",
    "    df.loc[df['question'].str.contains('How far away do you '), 'question'] = 'durCOV_prox'\n",
    "    df.loc[df['question'].str.contains('How long have you kn'), 'question'] = 'friendship_len'\n",
    "    df.loc[df['question'].str.contains('During-COVID \\(since '), 'question'] = 'durCOV_interact'\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Only if it's the current friend dataframe\n",
    "    if current:\n",
    "        df.loc[df['question'].str.contains('... 1-on-1 video cha'), 'question'] = 'ind_vid'\n",
    "        df.loc[df['question'].str.contains('... 1-on-1 text mess'), 'question'] = 'ind_txt'\n",
    "        df.loc[df['question'].str.contains('... 1-on-1 in-person'), 'question'] = 'ind_person'\n",
    "        df.loc[df['question'].str.contains('... group conference'), 'question'] = 'group_vid'\n",
    "        df.loc[df['question'].str.contains('... group text messa'), 'question'] = 'group_txt'\n",
    "        df.loc[df['question'].str.contains('... in-person \\(face-'), 'question'] = 'group_person'\n",
    "        df.loc[df['question'].str.contains('... other ways in a '), 'question'] = 'group_other'\n",
    "        df.loc[df['question'].str.contains('... other ways \\(spec'), 'question'] = 'ind_other'\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alter_boolean_df(stacked_ego_net_df, \n",
    "                            NG1_df, NG2_df, NG3_df, NG4_df, NG5_df, \n",
    "                            dorm_wing):    \n",
    "    \n",
    "    #dilter by dormwing\n",
    "    stacked_ego_net_df = stacked_edgelist[stacked_edgelist.dorm_wing == dorm_wing]\n",
    "    NG1_df = NG1_df[NG1_df.dorm_wing == dorm_wing]\n",
    "    NG2_df = NG2_df[NG2_df.dorm_wing == dorm_wing]\n",
    "    NG3_df = NG3_df[NG3_df.dorm_wing == dorm_wing]\n",
    "    NG4_df = NG4_df[NG4_df.dorm_wing == dorm_wing]\n",
    "    NG5_df = NG5_df[NG5_df.dorm_wing == dorm_wing]\n",
    "    \n",
    "    #get unique egos for each edge list. Will only check when ego is in the name generator\n",
    "    NG1_unique_egos = NG1_df['ego'].unique()\n",
    "    NG2_unique_egos = NG2_df['ego'].unique()\n",
    "    NG3_unique_egos = NG3_df['ego'].unique()\n",
    "    NG4_unique_egos = NG4_df['ego'].unique()\n",
    "    NG5_unique_egos = NG5_df['ego'].unique()\n",
    "\n",
    "    # loop to access all the pairs in a row\n",
    "    for index, row in stacked_ego_net_df.iterrows():\n",
    "\n",
    "        if row['ego'] in NG1_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG1_comp')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG2_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG2_comp')]= 1.0 \n",
    "\n",
    "        if row['ego'] in NG3_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG3_comp')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG4_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG4_comp')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG5_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG5_comp')]= 1.0\n",
    "\n",
    "\n",
    "    grouped_NG1 = NG1_df.groupby('ego')\n",
    "    grouped_NG2 = NG2_df.groupby('ego')\n",
    "    grouped_NG3 = NG3_df.groupby('ego')\n",
    "    grouped_NG4 = NG4_df.groupby('ego')\n",
    "    grouped_NG5 = NG5_df.groupby('ego')\n",
    "\n",
    "    # filter by dorm wing\n",
    "    NG1_df = NG1_df[(NG1_df.dorm_wing == dorm_wing)]\n",
    "    NG1_df = NG1_df.reset_index(drop = True)\n",
    "\n",
    "    NG2_df = NG2_df[(NG2_df.dorm_wing == dorm_wing)]\n",
    "    NG2_df = NG2_df.reset_index(drop = True)\n",
    "\n",
    "    NG3_df = NG3_df[(NG3_df.dorm_wing == dorm_wing)]\n",
    "    NG3_df = NG3_df.reset_index(drop = True)\n",
    "\n",
    "    NG4_df = NG4_df[(NG4_df.dorm_wing == dorm_wing)]\n",
    "    NG4_df = NG4_df.reset_index(drop = True)\n",
    "\n",
    "    NG5_df = NG5_df[(NG5_df.dorm_wing == dorm_wing)]\n",
    "    NG5_df = NG5_df.reset_index(drop = True)\n",
    "\n",
    "    # remove duplicates from stacked ego network\n",
    "    #stacked_ego_net_df = stacked_ego_net_df.drop_duplicates(ignore_index = True)\n",
    "\n",
    "    # loop to access all the pairs in a row\n",
    "    for index, row in stacked_ego_net_df.iterrows():\n",
    "\n",
    "        #Check if ego entered a name for NG1\n",
    "        if row['ego'] in NG1_unique_egos:\n",
    "            if row['alter'] in grouped_NG1.get_group(row['ego'])['alter'].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG1')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG2_unique_egos:\n",
    "            if row['alter'] in grouped_NG2.get_group(row['ego'])['alter'].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG2')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG3_unique_egos:\n",
    "            if row['alter'] in grouped_NG3.get_group(row['ego'])['alter'].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG3')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG4_unique_egos:\n",
    "            if row['alter'] in grouped_NG4.get_group(row['ego'])['alter'].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG4')]= 1.0             \n",
    "\n",
    "        if row['ego'] in NG5_unique_egos:\n",
    "            if row['alter'] in grouped_NG5.get_group(row['ego'])['alter'].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG5')]= 1.0\n",
    "\n",
    "    return(stacked_ego_net_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alter_boolean_df(stacked_ego_net_df, \n",
    "                            NG1_df, NG2_df, NG3_df, NG4_df, NG5_df, \n",
    "                            dorm_wing, alter_col = 'alter_id'):    \n",
    "    \n",
    "    #dilter by dormwing\n",
    "    stacked_ego_net_df = stacked_edgelist[stacked_edgelist.dorm_wing == dorm_wing]\n",
    "    NG1_df = NG1_df[NG1_df.dorm_wing == dorm_wing]\n",
    "    NG2_df = NG2_df[NG2_df.dorm_wing == dorm_wing]\n",
    "    NG3_df = NG3_df[NG3_df.dorm_wing == dorm_wing]\n",
    "    NG4_df = NG4_df[NG4_df.dorm_wing == dorm_wing]\n",
    "    NG5_df = NG5_df[NG5_df.dorm_wing == dorm_wing]\n",
    "    \n",
    "    #get unique egos for each edge list. Will only check when ego is in the name generator\n",
    "    NG1_unique_egos = NG1_df['ego'].unique()\n",
    "    NG2_unique_egos = NG2_df['ego'].unique()\n",
    "    NG3_unique_egos = NG3_df['ego'].unique()\n",
    "    NG4_unique_egos = NG4_df['ego'].unique()\n",
    "    NG5_unique_egos = NG5_df['ego'].unique()\n",
    "\n",
    "    # loop to access all the pairs in a row\n",
    "    for index, row in stacked_ego_net_df.iterrows():\n",
    "\n",
    "        if row['ego'] in NG1_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG1_comp')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG2_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG2_comp')]= 1.0 \n",
    "\n",
    "        if row['ego'] in NG3_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG3_comp')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG4_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG4_comp')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG5_unique_egos:\n",
    "                stacked_ego_net_df.at[index, ('NG5_comp')]= 1.0\n",
    "\n",
    "\n",
    "    grouped_NG1 = NG1_df.groupby('ego')\n",
    "    grouped_NG2 = NG2_df.groupby('ego')\n",
    "    grouped_NG3 = NG3_df.groupby('ego')\n",
    "    grouped_NG4 = NG4_df.groupby('ego')\n",
    "    grouped_NG5 = NG5_df.groupby('ego')\n",
    "\n",
    "    # filter by dorm wing\n",
    "    NG1_df = NG1_df[(NG1_df.dorm_wing == dorm_wing)]\n",
    "    NG1_df = NG1_df.reset_index(drop = True)\n",
    "\n",
    "    NG2_df = NG2_df[(NG2_df.dorm_wing == dorm_wing)]\n",
    "    NG2_df = NG2_df.reset_index(drop = True)\n",
    "\n",
    "    NG3_df = NG3_df[(NG3_df.dorm_wing == dorm_wing)]\n",
    "    NG3_df = NG3_df.reset_index(drop = True)\n",
    "\n",
    "    NG4_df = NG4_df[(NG4_df.dorm_wing == dorm_wing)]\n",
    "    NG4_df = NG4_df.reset_index(drop = True)\n",
    "\n",
    "    NG5_df = NG5_df[(NG5_df.dorm_wing == dorm_wing)]\n",
    "    NG5_df = NG5_df.reset_index(drop = True)\n",
    "\n",
    "    # remove duplicates from stacked ego network\n",
    "    #stacked_ego_net_df = stacked_ego_net_df.drop_duplicates(ignore_index = True)\n",
    "\n",
    "    # loop to access all the pairs in a row\n",
    "    for index, row in stacked_ego_net_df.iterrows():\n",
    "\n",
    "        #Check if ego entered a name for NG1\n",
    "        if row['ego'] in NG1_unique_egos:\n",
    "            if row[alter_col] in grouped_NG1.get_group(row['ego'])[alter_col].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG1')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG2_unique_egos:\n",
    "            if row[alter_col] in grouped_NG2.get_group(row['ego'])[alter_col].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG2')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG3_unique_egos:\n",
    "            if row[alter_col] in grouped_NG3.get_group(row['ego'])[alter_col].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG3')]= 1.0\n",
    "\n",
    "        if row['ego'] in NG4_unique_egos:\n",
    "            if row[alter_col] in grouped_NG4.get_group(row['ego'])[alter_col].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG4')]= 1.0             \n",
    "\n",
    "        if row['ego'] in NG5_unique_egos:\n",
    "            if row[alter_col] in grouped_NG5.get_group(row['ego'])[alter_col].unique():\n",
    "                stacked_ego_net_df.at[index, ('alter_NG5')]= 1.0\n",
    "\n",
    "    return(stacked_ego_net_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: General Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD YOUR CSV FILES\n",
    "\n",
    "# Change directory to where your CSVs are located\n",
    "os.chdir(BASE_DIR + \"working_directory/\")\n",
    "\n",
    "#raw data csv\n",
    "all_SNS2_df = pd.read_csv(\"2020may_data_wide.csv\")\n",
    "personal_info_df = pd.read_csv(\"2020may_info.csv\")\n",
    "friends_current_df = pd.read_csv(\"friends_current_long.csv\")\n",
    "friends_past_df = pd.read_csv(\"friends_past_long.csv\")\n",
    "thrd_party_df = pd.read_csv(\"friendship_long.csv\")\n",
    "likert_df = pd.read_csv(\"likert_long.csv\")\n",
    "\n",
    "#filter csv\n",
    "filter_df = pd.read_csv(\"filter_id.csv\")\n",
    "\n",
    "#ID dataframe\n",
    "id_df = pd.read_csv(\"id_names.csv\")\n",
    "\n",
    "#column renamer\n",
    "column_names_df = pd.read_csv(\"column_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER FILE NAME OUTPUTS\n",
    "sns2_filename = 'S1_all_SNS2_dataset.csv'\n",
    "friends_current_filename = 'S1_all_friends_current.csv'\n",
    "friends_past_filename = 'S1_all_friends_past.csv'\n",
    "thrd_party_filename = 'S1_all_ego_network.csv'\n",
    "likert_filename = 'S1_all_likert.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter personal dataframe\n",
    "print(\"PERSONAL\")\n",
    "print(personal_info_df.shape)\n",
    "personal_info_df = filter_this(filter_df, personal_info_df)\n",
    "print(personal_info_df.shape)\n",
    "\n",
    "print(\"ALL DATA\")\n",
    "#filter entire dataset\n",
    "print(all_SNS2_df.shape)\n",
    "all_SNS2_df = filter_this(filter_df, all_SNS2_df)\n",
    "print(all_SNS2_df.shape)\n",
    "\n",
    "print(\"CURRENT FRIENDS\")\n",
    "#filter current friends\n",
    "print(friends_current_df.shape)\n",
    "friends_current_df = filter_this(filter_df, friends_current_df)\n",
    "print(friends_current_df.shape)\n",
    "\n",
    "#filter past friends\n",
    "print(\"PAST FRIENDS\")\n",
    "print(friends_past_df.shape)\n",
    "friends_past_df = filter_this(filter_df, friends_past_df)\n",
    "print(friends_past_df.shape)\n",
    "\n",
    "#filter third party\n",
    "print(\"THIRD PARTY\")\n",
    "print(thrd_party_df.shape)\n",
    "thrd_party_df = filter_this(filter_df, thrd_party_df)\n",
    "print(thrd_party_df.shape)\n",
    "\n",
    "#filter likert dataset\n",
    "print(\"LIKERT DATASET\")\n",
    "print(likert_df.shape)\n",
    "likert_df = filter_this(filter_df, likert_df)\n",
    "print(likert_df.shape)\n",
    "\n",
    "#filter id dataset\n",
    "print(\"ID DATASET\")\n",
    "print(id_df.shape)\n",
    "id_df = filter_this(filter_df, id_df)\n",
    "print(id_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine personal dataframe w/ SNS2\n",
    "SNS2_df = pd.merge(personal_info_df, \n",
    "         all_SNS2_df, \n",
    "         how='outer', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine each response w/ their anonymous ids\n",
    "print(\"ALL DATA\")\n",
    "print(SNS2_df.shape)\n",
    "SNS2_df = pd.merge(id_df, \n",
    "         SNS2_df, \n",
    "         how='outer', on='id')\n",
    "print(SNS2_df.shape)\n",
    "\n",
    "print(\"FRIENDS CURRENT\")\n",
    "print(friends_current_df.shape)\n",
    "friends_current_df = pd.merge(id_df, \n",
    "         friends_current_df, \n",
    "         how='outer', on='id')\n",
    "print(friends_current_df.shape)\n",
    "      \n",
    "print(\"FRIENDS PAST\")\n",
    "print(friends_past_df.shape)\n",
    "friends_past_df = pd.merge(id_df, \n",
    "         friends_past_df, \n",
    "         how='outer', on='id')\n",
    "print(friends_past_df.shape)\n",
    "      \n",
    "print(\"THIRD PARTY\")\n",
    "print(thrd_party_df.shape)\n",
    "thrd_party_df = pd.merge(id_df, \n",
    "         thrd_party_df, \n",
    "         how='outer', on='id')\n",
    "print(thrd_party_df.shape)\n",
    "      \n",
    "print(\"LIKERT DATASET\")\n",
    "print(likert_df.shape)\n",
    "likert_df = pd.merge(id_df, \n",
    "         likert_df, \n",
    "         how='outer', on='id')\n",
    "print(likert_df.shape)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"friends current\")\n",
    "missing_friends_current = where_empty(friends_current_df, False, True)\n",
    "print('\\n')\n",
    "\n",
    "print(\"friends past\")\n",
    "missing_friends_past = where_empty(friends_past_df, False, True)\n",
    "print('\\n')\n",
    "\n",
    "print(\"likert\")\n",
    "missing_likert = where_empty(likert_df, False, True)\n",
    "print('\\n')\n",
    "\n",
    "print(\"third party\")\n",
    "missing_third_party = where_empty(thrd_party_df, False, True)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question_list = column_names_df[\"likert_q_equivalent\"].values.tolist()\n",
    "question_list = column_names_df[\"question_4_that_column\"].values.tolist()\n",
    "column_list = column_names_df[\"final_column_names\"].values.tolist()\n",
    "\n",
    "for i in range(len(question_list)):\n",
    "    likert_df['question'] = np.where(likert_df['question'] == question_list[i], column_list[i], likert_df['question'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your CSVs\n",
    "os.chdir(BASE_DIR + \"step1/\")\n",
    "\n",
    "SNS2_df.to_csv(sns2_filename, index = False)\n",
    "friends_current_df.to_csv(friends_current_filename, index = False)\n",
    "friends_past_df.to_csv(friends_past_filename, index = False)\n",
    "thrd_party_df.to_csv(thrd_party_filename, index = False)\n",
    "likert_df.to_csv(likert_filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create edge lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within Excel: from the S1_all_SNS2_dataset.csv, I copied the first 6 columns and each NG section to its own csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD YOUR FILES\n",
    "os.chdir(BASE_DIR + \"step1/\")\n",
    "\n",
    "social_current_df = pd.read_csv(\"current_social.csv\")\n",
    "virtual_current_df = pd.read_csv(\"current_virtual.csv\")\n",
    "social_media_df = pd.read_csv(\"social_media.csv\")\n",
    "aspire_df = pd.read_csv(\"aspire.csv\")\n",
    "social_precovid_df = pd.read_csv(\"precovid_social.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER FILE NAME OUTPUTS\n",
    "social_current_filename = 'S2_edgelist_social_current.csv'\n",
    "virtual_current_filename = 'S2_edgelist_virtual_current.csv'\n",
    "social_media_filename = 'S2_edgelist_social_media.csv'\n",
    "aspire_filename = 'S2_edgelist_aspire.csv'\n",
    "social_precovid_filename = 'S2_edgelist_social_precovid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = aspire_df[['fmri_wave1', 'ego', 'dorm_wing_x', 'firstname_x', 'lastname_x', 'id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the type of connection type for each edge list\n",
    "social_current_df = edgelist_creator(social_current_df)\n",
    "social_current_df['connection'] = np.where(social_current_df['connection'] == '2.current_q.0.names_in', 'dorm', social_current_df['connection'])\n",
    "social_current_df['connection'] = np.where(social_current_df['connection'] == '2.current_q.0.names_ou', 'outside', social_current_df['connection'])\n",
    "print(social_current_df.head())\n",
    "\n",
    "virtual_current_df = edgelist_creator(virtual_current_df)\n",
    "virtual_current_df['connection'] = np.where(virtual_current_df['connection'] == '2.current_q.1.names_in', 'dorm', virtual_current_df['connection'])\n",
    "virtual_current_df['connection'] = np.where(virtual_current_df['connection'] == '2.current_q.1.names_ou', 'outside', virtual_current_df['connection'])\n",
    "print(virtual_current_df.head())\n",
    "\n",
    "social_media_df = edgelist_creator(social_media_df)\n",
    "social_media_df['connection'] = np.where(social_media_df['connection'] == '2.initial.0.names_in_d', 'dorm', social_media_df['connection'])\n",
    "social_media_df['connection'] = np.where(social_media_df['connection'] == '2.initial.0.names_outs', 'outside', social_media_df['connection'])\n",
    "print(social_media_df.head())\n",
    "\n",
    "aspire_df = edgelist_creator(aspire_df)\n",
    "aspire_df['connection'] = np.where(aspire_df['connection'] == '2.initial.1.names_in_d', 'dorm', aspire_df['connection'])\n",
    "aspire_df['connection'] = np.where(aspire_df['connection'] == '2.initial.1.names_outs', 'outside', aspire_df['connection'])\n",
    "print(aspire_df.head())\n",
    "\n",
    "social_precovid_df = edgelist_creator(social_precovid_df)\n",
    "social_precovid_df['connection'] = np.where(social_precovid_df['connection'] == '2.past_q.0.names_in_do', 'dorm', social_precovid_df['connection'])\n",
    "social_precovid_df['connection'] = np.where(social_precovid_df['connection'] == '2.past_q.0.names_outsi', 'outside', social_precovid_df['connection'])\n",
    "print(social_precovid_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with id dataframe and identify people that did not complete that dataframe\n",
    "social_current_df = pd.merge(social_current_df, \n",
    "            id_df, \n",
    "            how='outer', on='ego')\n",
    "social_current_df['firstname_x'] = social_current_df['firstname_x'].str.lower()\n",
    "social_current_df['lastname_x'] = social_current_df['lastname_x'].str.lower()\n",
    "print(\"SOCIAL CURRENT\")\n",
    "missing_social_current = where_empty(social_current_df, False, True)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "social_precovid_df = pd.merge(social_precovid_df, \n",
    "            id_df, \n",
    "            how='outer', on='ego')\n",
    "social_precovid_df['firstname_x'] = social_precovid_df['firstname_x'].str.lower()\n",
    "social_precovid_df['lastname_x'] = social_precovid_df['lastname_x'].str.lower()\n",
    "print(\"SOCIAL PRECOVID\")\n",
    "missing_social_precovid = where_empty(social_precovid_df, False, True)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "aspire_df = pd.merge(aspire_df, \n",
    "            id_df, \n",
    "            how='outer', on='ego')\n",
    "aspire_df['firstname_x'] = aspire_df['firstname_x'].str.lower()\n",
    "aspire_df['lastname_x'] = aspire_df['lastname_x'].str.lower()\n",
    "print(\"ASPIRE\")\n",
    "missing_aspire = where_empty(aspire_df, False, True)\n",
    "print('\\n')\n",
    "\n",
    "social_media_df = pd.merge(social_media_df, \n",
    "            id_df, \n",
    "            how='outer', on='ego')\n",
    "social_media_df['firstname_x'] = social_media_df['firstname_x'].str.lower()\n",
    "social_media_df['lastname_x'] = social_media_df['lastname_x'].str.lower()\n",
    "print(\"SOCIAL MEDIA\")\n",
    "missing_social_media = where_empty(social_media_df, False, True)\n",
    "print('\\n')\n",
    "\n",
    "virtual_current_df = pd.merge(virtual_current_df, \n",
    "            id_df, \n",
    "            how='outer', on='ego')\n",
    "virtual_current_df['firstname_x'] = virtual_current_df['firstname_x'].str.lower()\n",
    "virtual_current_df['lastname_x'] = virtual_current_df['lastname_x'].str.lower()\n",
    "print(\"VIRTUAL\")\n",
    "missing_vurtual = where_empty(social_media_df, False, True)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove empty rows\n",
    "social_current_df = social_current_df.dropna(subset=['alter'])\n",
    "social_precovid_df = social_precovid_df.dropna(subset=['alter'])\n",
    "aspire_df = aspire_df.dropna(subset=['alter'])\n",
    "\n",
    "social_media_df = social_media_df.dropna(subset=['alter'])\n",
    "virtual_current_df = virtual_current_df.dropna(subset=['alter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your CSVs\n",
    "os.chdir(BASE_DIR + \"step2/\")\n",
    "\n",
    "social_current_df.to_csv(social_current_filename, index = False)\n",
    "social_precovid_df.to_csv(social_precovid_filename, index = False)\n",
    "aspire_df.to_csv(aspire_filename, index = False)\n",
    "social_media_df.to_csv(social_media_filename, index = False)\n",
    "virtual_current_df.to_csv(virtual_current_filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create nickname and mispelling dictionaries\n",
    "### Create lists of all unique names ever entered in each name generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD YOUR FILES\n",
    "os.chdir(BASE_DIR + \"step2/\")\n",
    "\n",
    "social_current_df = pd.read_csv(social_current_filename)\n",
    "virtual_current_df = pd.read_csv(virtual_current_filename)\n",
    "social_media_df = pd.read_csv(social_media_filename)\n",
    "aspire_df = pd.read_csv(aspire_filename)\n",
    "social_precovid_df = pd.read_csv(social_precovid_filename)\n",
    "\n",
    "# file for all dataset containing nicknames\n",
    "os.chdir(BASE_DIR + \"step1/\")\n",
    "all_SNS2_df = pd.read_csv('S1_all_SNS2_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER FILENAMES\n",
    "all_alters_filename = 'S3_alters_list.csv'\n",
    "nickname_filename = 'S3_nicknames_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe containing all unique alter spellings\n",
    "social_current_df = alter_anonymizer_df(social_current_df)\n",
    "social_precovid_df = alter_anonymizer_df(social_precovid_df)\n",
    "aspire_df = alter_anonymizer_df(aspire_df)\n",
    "social_media_df = alter_anonymizer_df(social_media_df)\n",
    "virtual_current_df = alter_anonymizer_df(virtual_current_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join all frames together\n",
    "frames = [social_current_df, social_precovid_df, aspire_df, social_media_df, virtual_current_df]\n",
    "all_alters_df = pd.concat(frames)\n",
    "\n",
    "#remove all duplicates\n",
    "all_alters_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nickname dictionary\n",
    "nickname_df = all_SNS2_df[['ego', 'lastname_x', '0.0.other_name', 'fmri_wave1', 'dorm_wing_x', 'id']].copy()\n",
    "\n",
    "#remove whitespaces\n",
    "nickname_df = nickname_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "#drop people who don't have a nickname\n",
    "nickname_df = nickname_df[nickname_df != str(0)].dropna()\n",
    "nickname_df = nickname_df.reset_index(drop=True)\n",
    "\n",
    "#lowercase nicknames and last names\n",
    "nickname_df['lastname'] = nickname_df['lastname_x'].str.lower()\n",
    "nickname_df['nickname_1'] = nickname_df['0.0.other_name'].str.lower()\n",
    "\n",
    "#drop old columns\n",
    "nickname_df = nickname_df.drop(columns=['0.0.other_name', 'lastname_x'])\n",
    "\n",
    "#split by \", \" and create new column for second nicknames\n",
    "new_data = nickname_df[\"nickname_1\"].str.split(\", \", n = 1, expand = True)\n",
    "nickname_df[\"nickname_1\"] = new_data[0]\n",
    "nickname_df[\"nickname_2\"] = new_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV files for each dataset\n",
    "os.chdir(BASE_DIR + \"step3/\")\n",
    "\n",
    "nickname_df.to_csv(nickname_filename, index = False)\n",
    "all_alters_df.to_csv(all_alters_filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Format long datasets\n",
    "##### S1_all_ego_network: \n",
    "create new columns of alter_1, alter_2, alter_1_connection, alter_2_connection (requires string splitting)\n",
    "\n",
    "##### S1_all_likert: \n",
    "transpose the dataframe so that new columns for each question, rename columns by index for columns that have weird string malfunctions\n",
    "\n",
    "##### S1_all_friends_current: \n",
    "create new columns of alter, alter_connection, question > transpose so that columns made for each question based on the start of the string \n",
    "##### S1_all_friends_past: \n",
    "create new columns of alter, alter_connection, question > transpose so that columns made for each question based on the start of the string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD YOUR FILES\n",
    "os.chdir(BASE_DIR + \"step1/\")\n",
    "\n",
    "friends_current_df = pd.read_csv(friends_current_filename)\n",
    "friends_past_df = pd.read_csv(friends_past_filename)\n",
    "third_party_df = pd.read_csv(thrd_party_filename)\n",
    "likert_df = pd.read_csv(likert_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER FILE NAME OUTPUTS\n",
    "os.chdir(BASE_DIR + \"step4/\")\n",
    "\n",
    "s4_sns2_filename = 'S4_all_SNS2_dataset.csv'\n",
    "s4_friends_current_filename = 'S4_all_friends_current.csv'\n",
    "s4_friends_past_filename = 'S4_all_friends_past.csv'\n",
    "s4_thrd_party_filename = 'S4_all_ego_network.csv'\n",
    "s4_likert_filename = 'S4_all_likert.csv'\n",
    "s4_specification_filename = 'S4_specification_all_friends_current.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likert dataset pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot your likert dataframe\n",
    "likert_df = likert_df.pivot(index = ['ego', 'dorm_wing', 'fmri_wave1', 'id'], columns = 'question', values='response')\n",
    "likert_df = likert_df.reset_index(drop = False)\n",
    "\n",
    "\n",
    "# Update column names that have weird strings\n",
    "column_names = likert_df.columns\n",
    "\n",
    "likert_df = likert_df.rename({column_names[4]: 'open_business', column_names[5]: 'skip_class', \n",
    "    column_names[6]: 'sleep_imp', column_names[7]: 'dining_hall', column_names[8]: 'friends_disagree', \n",
    "    column_names[9]: 'hungover_normal', column_names[10]: 'caffeine_normal', column_names[11]: 'alc_blackout',\n",
    "    column_names[12]: 'sleep_normal', column_names[13]: 'all_night_normal', column_names[14]: 'vape_use_all',\n",
    "    column_names[15]: 'vape_use_nicotine'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create new alter columns and remove unnecessary string characters\n",
    "friends_current_df, friends_past_df and ego networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# friends_current_df\n",
    "alter_df = alter_df_func(friends_current_df)\n",
    "\n",
    "friends_current_df['connection'] = alter_df['connection']\n",
    "friends_current_df['alter'] = alter_df['alter']\n",
    "friends_current_df['alter_id'] = alter_df['alter_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# friends_past_df\n",
    "alter_df = alter_df_func(friends_past_df)\n",
    "\n",
    "friends_past_df['connection'] = alter_df['connection']\n",
    "friends_past_df['alter'] = alter_df['alter']\n",
    "friends_past_df['alter_id'] = alter_df['alter_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for each alter column in your ego network dataset\n",
    "alter_df = alter_df_func(third_party_df, 'name1', 'alter1', 'alter1_id', 'alter1_connection')\n",
    "\n",
    "third_party_df['alter1_connection'] = alter_df['alter1_connection']\n",
    "third_party_df['alter1'] = alter_df['alter1']\n",
    "third_party_df['alter1_id'] = alter_df['alter1_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_df = alter_df_func(third_party_df, 'name2', 'alter2', 'alter2_id', 'alter2_connection')\n",
    "\n",
    "third_party_df['alter2_connection'] = alter_df['alter2_connection']\n",
    "third_party_df['alter2'] = alter_df['alter2']\n",
    "third_party_df['alter2_id'] = alter_df['alter2_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove unnecessary columns in ego network dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove ego network columns: 'name1', 'name2', 'firstname', 'lastname', 'timestamp\n",
    "third_party_df = third_party_df.drop(columns = ['name1', 'name2', 'firstname', 'lastname', 'timestamp'])\n",
    "\n",
    "# remove current and past columns: 'name', firstname', 'lastname', 'timestamp, 'response_text'\n",
    "friends_past_df = friends_past_df.drop(columns = ['name', 'firstname', 'lastname', 'timestamp', 'response_text'])\n",
    "friends_current_df = friends_current_df.drop(columns = ['name', 'firstname', 'lastname', 'timestamp', 'response_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rename each question for friends_past_df and friends_current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_current_df = rename_strings(friends_current_df, True)\n",
    "friends_past_df = rename_strings(friends_past_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create specification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specification csv file\n",
    "specification_df = friends_current_df[['fmri_wave1', 'ego', 'dorm_wing', 'id', 'question', 'specification', 'connection', 'alter', 'alter_id']].copy()\n",
    "specification_df = specification_df[specification_df != str(0)].dropna()\n",
    "specification_df = specification_df[specification_df.specification.notnull()]\n",
    "specification_df = specification_df[specification_df['specification'] != 'never']\n",
    "specification_df = specification_df[specification_df['specification'] != 'none']\n",
    "specification_df = specification_df[specification_df['specification'] != 'None']\n",
    "specification_df = specification_df[specification_df['specification'] != 'None ']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot your friends_past_df and friends_current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot your dataframes\n",
    "friends_current_df = friends_current_df.pivot(index = ['ego', 'dorm_wing', 'fmri_wave1', 'id', 'connection', 'alter_id', 'alter'], \n",
    "                               columns = ['question'], values = 'response')\n",
    "friends_current_df = friends_current_df.reset_index(drop = False)\n",
    "\n",
    "friends_past_df = friends_past_df.pivot(index = ['ego', 'dorm_wing', 'fmri_wave1', 'id', 'connection', 'alter_id', 'alter'], \n",
    "                               columns = ['question'], values = 'response')\n",
    "friends_past_df = friends_past_df.reset_index(drop = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CSV files for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(BASE_DIR + \"step4/\")\n",
    "\n",
    "third_party_df.to_csv(s4_thrd_party_filename, index = False)\n",
    "friends_current_df.to_csv(s4_friends_current_filename, index = False)\n",
    "friends_past_df.to_csv(s4_friends_past_filename, index = False)\n",
    "likert_df.to_csv(s4_likert_filename, index = False)\n",
    "specification_df.to_csv(s4_specification_filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Revise alters\n",
    "#### Revisions include: misspellings, nicknames, incorrect connection category\n",
    "##### Following datasets are revised: \n",
    "S2_edgelist_aspire.csv, S2_edgelist_social_current.csv, S2_edgelist_social_media.csv, S2_edgelist_social_precovid.csv, S2_edgelist_virtual_current.csv, S4_all_ego_network.csv, S4_all_friends_current.csv, S4_all_friends_past.csv, S4_specification_all_friends_current.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD YOUR FILES to revise\n",
    "\n",
    "# Edgelists\n",
    "os.chdir(BASE_DIR + \"step2/\")\n",
    "\n",
    "aspire_edgelist = pd.read_csv('S2_edgelist_aspire.csv')\n",
    "social_current_edgelist = pd.read_csv('S2_edgelist_social_current.csv')\n",
    "social_media_edgelist = pd.read_csv('S2_edgelist_social_media.csv')\n",
    "social_precovid_edgelist = pd.read_csv('S2_edgelist_social_precovid.csv')\n",
    "virtual_current_edgelist = pd.read_csv('S2_edgelist_virtual_current.csv')\n",
    "\n",
    "# Other datasets\n",
    "os.chdir(BASE_DIR + \"step4/\")\n",
    "\n",
    "ego_network_df = pd.read_csv('S4_all_ego_network.csv')\n",
    "friends_current_df = pd.read_csv('S4_all_friends_current.csv')\n",
    "friends_past_df = pd.read_csv('S4_all_friends_past.csv')\n",
    "specification_df = pd.read_csv('S4_specification_all_friends_current.csv')\n",
    "\n",
    "# Revision csv\n",
    "os.chdir(BASE_DIR + \"working_directory/\")\n",
    "\n",
    "revisions_df = pd.read_csv('revisions.csv')\n",
    "robert_revisions = pd.read_csv('s002_social_media_additions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filenames\n",
    "\n",
    "# edgelists\n",
    "s5_aspire_edgelist_filename = 'S5_aspire_edgelist.csv'\n",
    "s5_social_current_edgelist_filename = 'S5_social_current_edgelist.csv'\n",
    "s5_social_media_edgelist_filename = 'S5_social_media_edgelist.csv'\n",
    "s5_social_precovid_edgelist_filename = 'S5_social_precovid_edgelist.csv'\n",
    "s5_virtual_current_edgelist_filename = 'S5_virtual_current_edgelist.csv'\n",
    "\n",
    "# other datasets\n",
    "s5_ego_network_filename = 'S5_all_ego_network.csv'\n",
    "s5_friends_current_filename = 'S5_all_friends_current.csv'\n",
    "s5_friends_past_filename = 'S5_all_friends_past.csv'\n",
    "s5_specification_filename = 'S5_specification_all_friends_current.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add robert's alters to social media dataframe\n",
    "social_media_edgelist.update(robert_revisions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each edgelist, create alter_id column, rename dorm_wing_x column and remove unecessary columns\n",
    "\n",
    "alter_id_df = alter_anonymizer_df(aspire_edgelist)\n",
    "aspire_edgelist['alter_id'] = alter_id_df['alter_id']\n",
    "aspire_edgelist = aspire_edgelist.rename(columns = {'dorm_wing_x':'dorm_wing'})\n",
    "aspire_edgelist = aspire_edgelist.drop(columns = ['firstname_x', 'lastname_x'])\n",
    "\n",
    "alter_id_df = alter_anonymizer_df(social_current_edgelist)\n",
    "social_current_edgelist['alter_id'] = alter_id_df['alter_id']\n",
    "social_current_edgelist = social_current_edgelist.rename(columns = {'dorm_wing_x':'dorm_wing'})\n",
    "social_current_edgelist = social_current_edgelist.drop(columns = ['firstname_x', 'lastname_x'])\n",
    "\n",
    "alter_id_df = alter_anonymizer_df(social_media_edgelist)\n",
    "social_media_edgelist['alter_id'] = alter_id_df['alter_id']\n",
    "social_media_edgelist = social_media_edgelist.rename(columns = {'dorm_wing_x':'dorm_wing'})\n",
    "social_media_edgelist = social_media_edgelist.drop(columns = ['firstname_x', 'lastname_x'])\n",
    "\n",
    "alter_id_df = alter_anonymizer_df(social_precovid_edgelist)\n",
    "social_precovid_edgelist['alter_id'] = alter_id_df['alter_id']\n",
    "social_precovid_edgelist = social_precovid_edgelist.rename(columns = {'dorm_wing_x':'dorm_wing'})\n",
    "social_precovid_edgelist = social_precovid_edgelist.drop(columns = ['firstname_x', 'lastname_x'])\n",
    "\n",
    "alter_id_df = alter_anonymizer_df(virtual_current_edgelist)\n",
    "virtual_current_edgelist['alter_id'] = alter_id_df['alter_id']\n",
    "virtual_current_edgelist = virtual_current_edgelist.rename(columns = {'dorm_wing_x':'dorm_wing'})\n",
    "virtual_current_edgelist = virtual_current_edgelist.drop(columns = ['firstname_x', 'lastname_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise all the alters\n",
    "\n",
    "social_precovid_edgelist = revise_my_alters(social_precovid_edgelist, revisions_df)\n",
    "social_current_edgelist = revise_my_alters(social_current_edgelist, revisions_df)\n",
    "virtual_current_edgelist = revise_my_alters(virtual_current_edgelist, revisions_df)\n",
    "social_media_edgelist = revise_my_alters(social_media_edgelist, revisions_df)\n",
    "aspire_edgelist = revise_my_alters(aspire_edgelist, revisions_df)\n",
    "specification_df = revise_my_alters(specification_df, revisions_df)\n",
    "\n",
    "friends_current_df = revise_my_alters(friends_current_df, revisions_df)\n",
    "friends_past_df = revise_my_alters(friends_past_df, revisions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise alters in ego networks\n",
    "ego_network_df = revise_my_ego_netwrk_alters(ego_network_df, revisions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(BASE_DIR + \"step5/\")\n",
    "\n",
    "aspire_edgelist.to_csv(s5_aspire_edgelist_filename, index = False)\n",
    "social_current_edgelist.to_csv(s5_social_current_edgelist_filename, index = False)\n",
    "social_media_edgelist.to_csv(s5_social_media_edgelist_filename, index = False)\n",
    "social_precovid_edgelist.to_csv(s5_social_precovid_edgelist_filename, index = False)\n",
    "virtual_current_edgelist.to_csv(s5_virtual_current_edgelist_filename, index = False)\n",
    "\n",
    "friends_current_df.to_csv(s5_friends_current_filename, index = False)\n",
    "friends_past_df.to_csv(s5_friends_past_filename, index = False)\n",
    "ego_network_df.to_csv(s5_ego_network_filename, index = False)\n",
    "specification_df.to_csv(s5_specification_filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Boolean Datasets\n",
    "#### create a dataset w/ booleans values stating whether alter was entered for each name generator and whether ego completed that name generator\n",
    "##### COLUMNS: \n",
    "ego, dorm_wing, alter, alter_id, connection, aspire, social_current, social_media, social_precovid, virtual_current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD YOUR Edgelists\n",
    "\n",
    "os.chdir(BASE_DIR + \"step5/\")\n",
    "\n",
    "aspire_edgelist = pd.read_csv('S5_aspire_edgelist.csv')\n",
    "social_current_edgelist = pd.read_csv('S5_social_current_edgelist.csv')\n",
    "social_media_edgelist = pd.read_csv('S5_social_media_edgelist.csv')\n",
    "social_precovid_edgelist = pd.read_csv('S5_social_precovid_edgelist.csv')\n",
    "virtual_current_edgelist = pd.read_csv('S5_virtual_current_edgelist.csv')\n",
    "\n",
    "ego_network_df = pd.read_csv('S5_all_ego_network.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filenames\n",
    "S6_alter_boolean_filename = 'S6_alter_boolean.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin alter_boolean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacked edgelist containing all unique alters entered for each ego\n",
    "edgelist_list = [aspire_edgelist, social_current_edgelist, social_media_edgelist, social_precovid_edgelist, virtual_current_edgelist]  # List of your dataframes\n",
    "stacked_edgelist = pd.concat(edgelist_list)\n",
    "\n",
    "#remove duplicates based on alter_id\n",
    "stacked_edgelist = stacked_edgelist.drop_duplicates(['dorm_wing', 'ego','alter_id', 'connection'], keep= 'last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked_ego_net_df\n",
    "south_alter_df = create_alter_boolean_df(stacked_edgelist, \n",
    "                             aspire_edgelist, social_current_edgelist, social_media_edgelist,\n",
    "                            social_precovid_edgelist, virtual_current_edgelist, 'south')\n",
    "\n",
    "north_alter_df = create_alter_boolean_df(stacked_edgelist, \n",
    "                             aspire_edgelist, social_current_edgelist, social_media_edgelist,\n",
    "                            social_precovid_edgelist, virtual_current_edgelist, 'north')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both datasets\n",
    "alter_df = pd.merge(north_alter_df, \n",
    "             south_alter_df, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only necessary columns and fill w/ 0s\n",
    "alter_boolean_df = alter_df[['ego', 'alter', 'connection', 'fmri_wave1', 'dorm_wing', 'id',\n",
    "       'alter_id', 'alter_NG1', 'alter_NG2', 'alter_NG3', 'alter_NG4', 'alter_NG5']].copy()\n",
    "\n",
    "alter_boolean_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ego boolean dataset\n",
    "This dataset that reports whether an ego entered a name for each name generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_boolean_df = alter_df[['ego', 'fmri_wave1', 'dorm_wing', 'id',\n",
    "       'NG1_comp', 'NG2_comp', 'NG3_comp', 'NG4_comp', 'NG5_comp']].copy()\n",
    "\n",
    "\n",
    "ego_boolean_df = ego_boolean_df.drop_duplicates(ignore_index = True)\n",
    "ego_boolean_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename your columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_boolean_df = ego_boolean_df.rename({'NG1_comp': 'aspire', 'NG2_comp': 'social_current', \n",
    "                                'NG3_comp': 'social_media', 'NG4_comp': 'social_precovid',\n",
    "                                'NG5_comp': 'virtual_current'}, axis=1)\n",
    "\n",
    "\n",
    "alter_boolean_df = alter_boolean_df.rename({'alter_NG1': 'aspire', 'alter_NG2': 'social_current', \n",
    "                                'alter_NG3': 'social_media', 'alter_NG4': 'social_precovid',\n",
    "                                'alter_NG5': 'virtual_current'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(BASE_DIR + \"step6/\")\n",
    "\n",
    "ego_boolean_df.to_csv('S6_ego_boolean.csv', index = False)\n",
    "alter_boolean_df.to_csv('S6_alter_boolean.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final clean up\n",
    "Check if there are any alters that might actually be different people\n",
    "\n",
    "Average ratings that are for the same alter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD YOUR DATASETS\n",
    "os.chdir(BASE_DIR + \"step5/\")\n",
    "\n",
    "aspire_edgelist = pd.read_csv('S5_aspire_edgelist.csv')\n",
    "social_current_edgelist = pd.read_csv('S5_social_current_edgelist.csv')\n",
    "social_media_edgelist = pd.read_csv('S5_social_media_edgelist.csv')\n",
    "social_precovid_edgelist = pd.read_csv('S5_social_precovid_edgelist.csv')\n",
    "virtual_current_edgelist = pd.read_csv('S5_virtual_current_edgelist.csv')\n",
    "\n",
    "friends_current_df = pd.read_csv('S5_all_friends_current.csv')\n",
    "friends_past_df = pd.read_csv('S5_all_friends_past.csv')\n",
    "\n",
    "ego_net_df = pd.read_csv('S5_all_ego_network.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filenames\n",
    "\n",
    "# EDGELISTS\n",
    "aspire_edgelist_filename = 'S7_edgelist_aspire.csv'\n",
    "social_current_filename = 'S7_edgelist_social_current.csv'\n",
    "social_media_filename = 'S7_edgelist_social_media.csv'\n",
    "social_precovid_filename = 'S7_edgelist_social_precovid.csv'\n",
    "virtual_current_filename = 'S7_edgelist_virtual_current.csv'\n",
    "\n",
    "# RATINGS\n",
    "friendship_ratings_filename = 'S7_friendship_ratings.csv'\n",
    "unmerged_friends_current_filename = 'S7_friendship_current_ratings.csv'\n",
    "unmerged_friends_past_filename = 'S7_friendship_past_ratings.csv'\n",
    "\n",
    "# EGO NETWORK\n",
    "ego_net_df_filename = 'S7_ego_network.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average ratings for alters that are the same but entered w/ mispellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average ratings for same alters in closeness/interaction/modality ratings\n",
    "friend_current_col = friends_current_df.columns[7:]\n",
    "for col in friend_current_col:\n",
    "    friends_current_df[col] = friends_current_df.groupby(['dorm_wing', 'ego', 'connection', 'alter_id'])[col].transform('mean')\n",
    "\n",
    "friend_past_col = friends_past_df.columns[7:]\n",
    "for col in friend_past_col:\n",
    "    friends_past_df[col] = friends_past_df.groupby(['dorm_wing', 'ego', 'connection', 'alter_id'])[col].transform('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmerged_friends_past_df = friends_past_df\n",
    "unmerged_friends_past_df = unmerged_friends_past_df.drop_duplicates(['dorm_wing', 'ego', 'connection', 'alter_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmerged_friends_current_df = friends_current_df\n",
    "unmerged_friends_current_df = unmerged_friends_current_df.drop_duplicates(['dorm_wing', 'ego', 'connection', 'alter_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out each shape\n",
    "print(aspire_edgelist.shape)\n",
    "print(social_current_edgelist.shape)\n",
    "print(social_media_edgelist.shape)\n",
    "print(social_precovid_edgelist.shape)\n",
    "print(virtual_current_edgelist.shape)\n",
    "#print(friends_current_df.shape)\n",
    "#print(friends_past_df.shape)\n",
    "print(ego_net_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates in each dataset\n",
    "aspire_edgelist = aspire_edgelist.drop_duplicates(['dorm_wing', 'ego', 'alter_id', 'connection'], keep= 'last')\n",
    "social_current_edgelist = social_current_edgelist.drop_duplicates(['dorm_wing', 'ego', 'alter_id', 'connection'], keep= 'last')\n",
    "social_media_edgelist = social_media_edgelist.drop_duplicates(['dorm_wing', 'ego', 'alter_id', 'connection'], keep= 'last')\n",
    "social_precovid_edgelist = social_precovid_edgelist.drop_duplicates(['dorm_wing', 'ego', 'alter_id', 'connection'], keep= 'last')\n",
    "virtual_current_edgelist = virtual_current_edgelist.drop_duplicates(['dorm_wing', 'ego', 'alter_id', 'connection'], keep= 'last')\n",
    "#friends_current_df = friends_current_df.drop_duplicates(['dorm_wing', 'ego', 'alter_id', 'connection'], keep= 'last')\n",
    "#friends_past_df = friends_past_df.drop_duplicates(['dorm_wing', 'ego', 'alter_id', 'connection'], keep= 'last')\n",
    "ego_net_df = ego_net_df.drop_duplicates(['dorm_wing', 'ego', 'alter1_id', 'alter2_id','alter1_connection', 'alter2_connection'], keep= 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out resulting shape\n",
    "print('\\n')\n",
    "print(aspire_edgelist.shape)\n",
    "print(social_current_edgelist.shape)\n",
    "print(social_media_edgelist.shape)\n",
    "print(social_precovid_edgelist.shape)\n",
    "print(virtual_current_edgelist.shape)\n",
    "#print(friends_current_df.shape)\n",
    "#print(friends_past_df.shape)\n",
    "print(ego_net_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create your final files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(BASE_DIR + \"step7/\")\n",
    "\n",
    "# EDGELISTS\n",
    "aspire_edgelist.to_csv(aspire_edgelist_filename, index = False)\n",
    "social_current_edgelist.to_csv(social_current_filename, index = False)\n",
    "social_media_edgelist.to_csv(social_media_filename, index = False)\n",
    "social_precovid_edgelist.to_csv(social_precovid_filename, index = False)\n",
    "virtual_current_edgelist.to_csv(virtual_current_filename, index = False)\n",
    "\n",
    "# RATINGS\n",
    "#friends_current_df.to_csv(friendship_ratings_filename, index = False)\n",
    "unmerged_friends_current_df.to_csv(unmerged_friends_current_filename, index = False)\n",
    "unmerged_friends_past_df.to_csv(unmerged_friends_past_filename, index = False)\n",
    "\n",
    "# EGO NETWORK\n",
    "ego_net_df.to_csv(ego_net_df_filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7b: Average the ratings in a merged dataset\n",
    "Manually, combine both the current and long friendship datasets from step 5 and store in working_directory folder. Run r script \"step7_friendship_ratings.R\" for your final merged friendship ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA: Checking long friendship datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "os.chdir(BASE_DIR + \"step5/\")\n",
    "\n",
    "friends_current_df = pd.read_csv('S5_all_friends_current.csv')\n",
    "friends_past_df = pd.read_csv('S5_all_friends_past.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge both datasets\n",
    "new_df = pd.merge(friends_current_df, friends_past_df,\n",
    "                  how = 'outer',\n",
    "                  on = ['ego', 'dorm_wing', 'fmri_wave1', 'id', 'connection', 'alter_id','alter'],\n",
    "                  suffixes=('_current', '_past'))\n",
    "\n",
    "new_df['conflict_ratings'] = new_df.duplicated(['ego', 'dorm_wing', 'fmri_wave1', 'id', 'connection', 'alter'], keep = False)\n",
    "\n",
    "new_df['alter_lookup'] = new_df[\"ego\"] + new_df[\"alter\"] + new_df[\"connection\"] + new_df[\"dorm_wing\"]\n",
    "new_df['alter_id_lookup'] = new_df[\"ego\"] + new_df[\"alter_id\"] + new_df[\"connection\"] + new_df[\"dorm_wing\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.reindex(sorted(new_df.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(BASE_DIR + \"extra/\")\n",
    "new_df.to_csv('merged_friendship_rating_comparisons_UNORDERED_COLUMNS.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('merged')\n",
    "print(len(np.unique(new_df['alter_id_lookup'])))\n",
    "print(len(np.unique(new_df['alter_lookup'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA: Create ego and alter boolean datasets based on raw entries (column \"alter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD YOUR Edgelists\n",
    "\n",
    "os.chdir(BASE_DIR + \"step5/\")\n",
    "\n",
    "aspire_edgelist = pd.read_csv('S5_aspire_edgelist.csv')\n",
    "social_current_edgelist = pd.read_csv('S5_social_current_edgelist.csv')\n",
    "social_media_edgelist = pd.read_csv('S5_social_media_edgelist.csv')\n",
    "social_precovid_edgelist = pd.read_csv('S5_social_precovid_edgelist.csv')\n",
    "virtual_current_edgelist = pd.read_csv('S5_virtual_current_edgelist.csv')\n",
    "\n",
    "ego_network_df = pd.read_csv('S5_all_ego_network.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filenames\n",
    "extra_alter_boolean_filename = 'extra_alter_boolean.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacked edgelist containing all unique alters entered for each ego\n",
    "edgelist_list = [aspire_edgelist, social_current_edgelist, social_media_edgelist, social_precovid_edgelist, virtual_current_edgelist]  \n",
    "stacked_edgelist = pd.concat(edgelist_list)\n",
    "\n",
    "#remove duplicates based on alter\n",
    "stacked_edgelist = stacked_edgelist.drop_duplicates(['dorm_wing', 'ego','alter', 'connection'], keep= 'last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_alter_df = create_alter_boolean_df(stacked_edgelist, \n",
    "                             aspire_edgelist, social_current_edgelist, social_media_edgelist,\n",
    "                            social_precovid_edgelist, virtual_current_edgelist, 'south', 'alter')\n",
    "\n",
    "north_alter_df = create_alter_boolean_df(stacked_edgelist, \n",
    "                             aspire_edgelist, social_current_edgelist, social_media_edgelist,\n",
    "                            social_precovid_edgelist, virtual_current_edgelist, 'north', 'alter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both datasets\n",
    "alter_df = pd.merge(north_alter_df, \n",
    "             south_alter_df, how = 'outer')\n",
    "\n",
    "# Keep only necessary columns and fill w/ 0s\n",
    "alter_df = alter_df[['ego', 'alter', 'connection', 'fmri_wave1', 'dorm_wing', 'id',\n",
    "       'alter_id', 'alter_NG1', 'alter_NG2', 'alter_NG3', 'alter_NG4', 'alter_NG5']].copy()\n",
    "\n",
    "alter_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_df = alter_df.rename({'alter_NG1': 'aspire', 'alter_NG2': 'social_current', \n",
    "                                'alter_NG3': 'social_media', 'alter_NG4': 'social_precovid',\n",
    "                                'alter_NG5': 'virtual_current'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new files\n",
    "os.chdir(BASE_DIR + \"extra/\")\n",
    "\n",
    "alter_df.to_csv(extra_alter_boolean_filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
